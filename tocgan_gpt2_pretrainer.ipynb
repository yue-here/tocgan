{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to pretrain GPT2 on chemistry paper titles\n",
    "Pretrain GPT2 to use as the decoder component of a vision encoder-decoder for 'this JACS does not exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check memory usage with !nvidia-smi (if running in a notebook, the kernel needs to be restarted to clear the memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate training dataframes from a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import os\n",
    "\n",
    "# Load the csv\n",
    "headings = ['title', 'file_name', 'Abstract']\n",
    "df = pd.read_csv('data.csv', names = headings)\n",
    "\n",
    "# Drop null values\n",
    "df = df.mask(df.eq('None')).dropna()\n",
    "\n",
    "# Data was written as byte strings of the form b'string' - evaluate in python then decode to utf-8\n",
    "df['title'] = df['title'].map(lambda x: literal_eval(x).decode('utf-8'))\n",
    "\n",
    "# Make train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a histogram of the lengths of the tokenized titles to find what the max length cutoff is. (For this dataset 128 was plenty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'].map(lambda x: len(text_processor(x)['input_ids'])).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2 usually does not use padding tokens, only \"<|endoftext|>\". We need to add a padding token to the tokenizer so that model can distinguish between padding and end-of-sequence (EOS) tokens to learn end of sequence positions (i.e. to learn that titles should not be too long)\n",
    "\n",
    "**Important:** Pretrained models' token embedding sizes will need to be resized to match the new tokenizer embedding size: use `model.resize_token_embeddings(len(new_vocab))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "text_processor = GPT2Tokenizer.from_pretrained(\"gpt2\", pad_token=\"<|pad|>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a torch dataset that loads our text dataset. \n",
    "\n",
    "We add an EOS token to the end of each title, then pad with the padding tokens defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, text_processor, max_target_len=128):\n",
    "        self.df = df\n",
    "        self.text_processor = text_processor\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Add the EOS token\n",
    "        title = self.df['title'][idx] + self.text_processor.eos_token\n",
    "        text = self.text_processor(\n",
    "            title,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_target_len,\n",
    "            truncation=True, # Need to set this as it does not auto-truncate\n",
    "            ).input_ids\n",
    "\n",
    "        return torch.tensor(text, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 57452\n",
      "Number of eval samples: 6384\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "    df=train_df,\n",
    "    text_processor=text_processor,\n",
    "    max_target_len=128)\n",
    "\n",
    "eval_dataset = TextDataset(\n",
    "    df=test_df,\n",
    "    text_processor=text_processor,\n",
    "    max_target_len=128)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of eval samples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check tokenization works properly - note the EOS token 50256 followed by multiple 50257 padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   77, 39310, 46582,     9,  4225,  4658,  3401,  5039,   262,  3167,\n",
       "         4754,   485, 33396, 32480,   286,  4551,   312,   342,  2101,  1134,\n",
       "          316,   404,  9346, 15742, 50256, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "      tokenizer=text_processor, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Initialise the parameters for the Huggingface transformers trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "# Load a pre-trained vanilla GPT2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Resize model token embeddings to account for extra padding token\n",
    "model.resize_token_embeddings(len(text_processor))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./<OUTPUT-DIR>\", # output directory\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=800, # number of training epochs\n",
    "    per_device_train_batch_size=16, # batch size for training\n",
    "    per_device_eval_batch_size=16, # batch size for evaluation\n",
    "    evaluation_strategy='steps', # which pattern to use for evaluation\n",
    "    eval_steps=1000, # number of steps between two evaluations\n",
    "    save_steps=10000, # number of steps between model saves \n",
    "    save_total_limit=3, # maximum number of models to keep (oldest ones are removed)\n",
    "    warmup_steps=500, # number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "# Pass arguments to trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the trainer!\n",
    "\n",
    "**Note:** this is better run as a python script than a notebook, the cell output can become very long leading to crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the model to local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./gpt2-pretrain\n",
      "Configuration saved in ./gpt2-pretrain\\config.json\n",
      "Model weights saved in ./gpt2-pretrain\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./gpt2-pretrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try loading a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-pretrain3\\checkpoint-20000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generate function (can also use the transformers `model.generate()` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_titles = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = generate(model.to('cpu'), text_processor, test_data['title'][i], entry_count=1)\n",
    "    generated_titles.append(x)\n",
    "  return generated_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model by generating prompts from the first 4 words from training set entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "test_set = test_df['title'].sample(n=10)\n",
    "test_set_prompt = test_set.str.split().str[:4].str.join(' ')\n",
    "test_set_prompt = test_set_prompt.reset_index()\n",
    "test_set_prompt\n",
    "\n",
    "#Run the functions to generate the titles\n",
    "generated_titles = text_generation(test_set_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare generated titles to original titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Origin of Dark-Channel X-ray Absorption Fine Structure of (NH3)4Ca(OH)5 in Supercritical Carbon Dioxide<|endoftext|>']\n",
      "['Comprehensive Thermochemistry of W–H and H–H Bonds in the Lanthanide Phosphate (Ln5Me4) System<|endoftext|>']\n",
      "['Fragmentation Energetics of Clusters Based on Cluster Modification:\\u2009 Assignment of the Concentration-Dependent Rate Constant<|endoftext|>']\n",
      "['Transient Photoconductivity of Acceptor-Substituted Layered Zirconium Oxides<|endoftext|>']\n",
      "['Palladium-Catalyzed Aerobic Oxidative Cyclization of Unactivated Alkenes<|endoftext|>']\n",
      "['Mild Aerobic Oxidative Palladium(II)-Catalyzed Arylation of Indoles:\\u2009 Access to Chiral Olefins<|endoftext|>']\n",
      "['A Pentacoordinate Boron-Containing π-Electron System for High-Performance Polymer Solar Cells<|endoftext|>']\n",
      "['Ferroelectric Alkylamide-Substituted Helicene Derivative: Synthesis, Characterization, and Redox Properties<|endoftext|>']\n",
      "['Tandem Cyclopropanation/Ring-Closing Metathesis of Cyclohexadienes: Convergent Access to Optically Active α-Hydroxy Esters<|endoftext|>']\n",
      "['Cyclic Penta-Twinned Rhodium Nanobranches:\\u2009 Isolation, Structural Characterization, and Catalytic Activity<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "for i in generated_titles:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin of Dark-Channel X-ray Fluorescence from Transition-Metal Ions in Water\n",
      "Comprehensive Thermochemistry of W–H Bonding in the Metal Hydrides CpW(CO)2(IMes)H, [CpW(CO)2(IMes)H]•+, and [CpW(CO)2(IMes)(H)2]+. Influence of an N-Heterocyclic Carbene Ligand on Metal Hydride Bond Energies\n",
      "Fragmentation Energetics of Clusters Relevant to Atmospheric New Particle Formation\n",
      "Transient Photoconductivity of Acceptor-Substituted Poly(3-butylthiophene)\n",
      "Palladium-Catalyzed Aerobic Oxidative Cyclization of N-Aryl Imines: Indole Synthesis from Anilines and Ketones\n",
      "Mild Aerobic Oxidative Palladium (II) Catalyzed C−H Bond Functionalization:  Regioselective and Switchable C−H Alkenylation and Annulation of Pyrroles\n",
      "A Pentacoordinate Boron-Containing π-Electron System with Cl–B–Cl Three-Center Four-Electron Bonds\n",
      "Ferroelectric Alkylamide-Substituted Helicene Derivative with Two-Dimensional Hydrogen-Bonding Lamellar Phase\n",
      "Tandem Cyclopropanation/Ring-Closing Metathesis of Dienynes\n",
      "Cyclic Penta-Twinned Rhodium Nanobranches as Superior Catalysts for Ethanol Electro-oxidation\n"
     ]
    }
   ],
   "source": [
    "for i in test_set:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf7f35d8ebf284349bb5a3f392dfedeaab9cb236cac86ce1ffb0fbd472ccf58a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
