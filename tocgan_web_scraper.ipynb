{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraper for ACS journals**\n",
    "\n",
    "Make imports\n",
    "* BeautifulSoup - parsing html\n",
    "* undetected_chromedriver - headless browser\n",
    "\n",
    "Set the root url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "root = \"https://pubs.acs.org/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(root, stem):\n",
    "    '''\n",
    "    Download image from the url root + stem\n",
    "    '''\n",
    "\n",
    "    url = urljoin(root, stem)\n",
    "\n",
    "    # Get the image\n",
    "    response = requests.get(url)\n",
    "    _, file_ = os.path.split(stem)\n",
    "\n",
    "    # Check 'images' folder exists\n",
    "    if not os.path.exists(\"images\"):\n",
    "        os.mkdir(\"images\")\n",
    "\n",
    "    # Write image to file\n",
    "    out_path = os.path.join(\"images\", file_)\n",
    "    with open(out_path, \"wb\") as out_file:\n",
    "        out_file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Direct download**\n",
    "* Loop through all issues of the journal\n",
    "* Parse the html to find the location of the ToC image\n",
    "* Download the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ranges to cover the issues for the journal of interest\n",
    "for i in range(144, 0, -1):\n",
    "    for j in range(52, 0, -1):\n",
    "        \n",
    "        # Substitute <JOURNAL URL>\n",
    "        URL = f\"https://pubs.acs.org/toc/<JOURNAL URL>/{i}/{j}\"\n",
    "\n",
    "        page = requests.get(URL)\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        results = soup.find_all(\"img\", class_ = \"lazy\")\n",
    "\n",
    "        for result in results:\n",
    "            download_image(root, result[\"data-src\"]) # updated to src\n",
    "            print(result[\"data-src\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If a headless browser is needed (e.g. for cloudflare protection)**\n",
    "* This example is for downloading text from specific classes in the html\n",
    "* e.g. Paper titles and abstracts. \n",
    "* **Note:** if using text with nonstandard characters, check encoding\n",
    "* Write to <FILENAME.csv>\n",
    "* Could also download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open ('<FILENAME>.csv', 'a') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for i in range(144, 0, -1):\n",
    "        for j in range(52, 0, -1):\n",
    "            URL = f\"https://pubs.acs.org/toc/<JOURNAL URL>/{i}/{j}\"\n",
    "            # print(f'Volume {i} Issue {j}')\n",
    "\n",
    "            # Access page via headless chrome driver\n",
    "            options = uc.ChromeOptions()\n",
    "            options.headless = True\n",
    "            driver = uc.Chrome(options=options)\n",
    "            driver.get(URL)\n",
    "            page = driver.page_source\n",
    "\n",
    "            # Parse the html\n",
    "            soup = BeautifulSoup(page, \"html.parser\")\n",
    "            results = soup.find_all(\"div\", class_ = \"issue-item clearfix\")\n",
    "\n",
    "            # Find the appropriate content within the html\n",
    "            # title, image url (for matching image filename), and abstract\n",
    "            for k in results:\n",
    "                title = k.a.get('title').encode(\"utf-8\")\n",
    "                if k.img:\n",
    "                    img = k.img.get('data-src')\n",
    "                else:\n",
    "                    img = 'None'\n",
    "                if k.p:\n",
    "                    abs = k.p.getText().encode(\"utf-8\")\n",
    "                else:\n",
    "                    abs = 'None'\n",
    "                writer.writerow([title, img, abs])\n",
    "\n",
    "    # Close the headless browser\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa12b7b1b98fc1a6e606b3008928e0a217c5acbfad9aa50ef719648704c150df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('webscraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
